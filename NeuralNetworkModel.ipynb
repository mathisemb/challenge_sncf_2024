{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from utils.PandasToolsFunction import *\n",
    "from utils.DataPreprocessingTools import *\n",
    "from utils.loss_functions import mean_absolute_percentage_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_station_split(train_data_init: pd.DataFrame, size = 50, station_col ='station'):\n",
    "    train_data = train_data_init.copy()\n",
    "\n",
    "    train_data.sort_values(station_col, inplace=True)\n",
    "    stations = train_data[station_col].unique()\n",
    "    total_stations = len(stations)\n",
    "\n",
    "    station_count =0\n",
    "    split_dataframes = []\n",
    "    end = False\n",
    "    \n",
    "    while not end :\n",
    "        if station_count + size <= total_stations :\n",
    "            current_stations = stations[station_count: station_count + size]\n",
    "        else : \n",
    "            current_stations = stations[station_count:]\n",
    "            end = True\n",
    "\n",
    "        station_count += size\n",
    "        stations_filter = make_station_filter(train_data, current_stations)\n",
    "        current_dataframe = train_data[stations_filter]\n",
    "        split_dataframes.append(current_dataframe)\n",
    "\n",
    "    return split_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# Define the MLP regression model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, n_layers = 2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layers = torch.nn.ModuleList([torch.nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers - 1)])\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModelPytorch():\n",
    "    \"\"\"\n",
    "    Split data per stations to not overcharge memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data: pd.DataFrame, size = 50, model_type='MLP', hidden_dim =32, n_layers =2, station_col = 'station', features_col = ['week', 'day_numeric', 'day_type'], label_col = 'y') -> None:\n",
    "\n",
    "        self.submission = None\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.station_col = station_col\n",
    "        self.features_col = features_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "        self.features_dim = len(features_col)\n",
    "\n",
    "        self.setting_scaler(train_data)\n",
    "\n",
    "        # We split the training date per subsets of \"size\" stations each.\n",
    "        self.split_dataframes = train_data_station_split(self.train_data, size, station_col)\n",
    "    \n",
    "    def setting_scaler(self, train_data: pd.DataFrame):\n",
    "                \n",
    "        # We need to use only one scaler for every station, we fit on one station and we then reuse it for every other. \n",
    "        # Be careful to use the same for testing than for training, indeed, as there are not as many weeks on the test, set, the scaling would be different\n",
    "        # if it is made on the test.set\n",
    "\n",
    "        self.scaler = MinMaxScaler()\n",
    "        # Training the scaler once and for all :\n",
    "\n",
    "        mask = (train_data['station'] == train_data[self.station_col].unique()[0])\n",
    "        features = train_data[mask][self.features_col]\n",
    "        _ = self.scaler.fit_transform(features)\n",
    "\n",
    "    def preprocessing_data(self, small_train_data: pd.DataFrame, test_size = 0.01, lr= 0.01, Optim = optim.SGD):\n",
    "\n",
    "        stations = small_train_data[self.station_col].unique()\n",
    "\n",
    "        # Creating one model per station. \n",
    "        if self.model_type == 'MLP':\n",
    "            modules=[MLP(self.features_dim, hidden_dim=self.hidden_dim, n_layers=self.n_layers) for _ in range(len(stations))]\n",
    "        \n",
    "        self.models = {station : (module, Optim(params=module.parameters(), lr=lr) ) for station, module in zip(stations, modules) } \n",
    "        self.datasets = {} # each dataset will contain four sets : X_train, X_test, Y_train, Y_test\n",
    "\n",
    "        # creating datasets\n",
    "\n",
    "        for station in stations :\n",
    "\n",
    "            station_indices = small_train_data.index[small_train_data[self.station_col] == station]\n",
    "            features = small_train_data.loc[station_indices, self.features_col]\n",
    "            y = small_train_data.loc[station_indices, self.label_col]\n",
    "            # print(\"TYPE Y\", type(y)) # pandas.Series\n",
    "            y = np.array(y)\n",
    "\n",
    "            X_normalized = self.scaler.transform(features)\n",
    "\n",
    "            self.datasets[station] = train_test_split(X_normalized, y, test_size=test_size, random_state= 42, shuffle=False)\n",
    "\n",
    "    def train(self, epochs = 10, test_size = 0.01, lr = 0.01, Optim = optim.SGD, print_period: int = 1000, threshold = 0.1) : #, train_data: pd.DataFrame, features_col = ['week', 'day_numeric', 'day_type'], label_col = 'y'):\n",
    "\n",
    "        for k, small_train_data in tqdm(enumerate(self.split_dataframes), desc=\"Split dataframes\") :\n",
    "            \n",
    "            self.preprocessing_data(small_train_data, test_size, lr, Optim)\n",
    "\n",
    "            for station, model in tqdm(self.models.items(), desc=\"Stations\"):\n",
    "                \n",
    "                module, optimizer = model\n",
    "                X_train, X_test, y_train, y_test = self.datasets[station]\n",
    "\n",
    "                scheduler = StepLR(optimizer,\n",
    "                   step_size = 1,\n",
    "                   gamma = 0.5)\n",
    "\n",
    "                scheduler_period = 10\n",
    "\n",
    "                # Converting to tensors and putting them on the good device\n",
    "                X_train, X_test = torch.Tensor(X_train, device=device), torch.Tensor(X_test, device=device)\n",
    "                y_train, y_test = torch.Tensor(y_train, device=device), torch.Tensor(y_test, device=device)  \n",
    "                \n",
    "                module = module.to(device)\n",
    "                \n",
    "                #print(X_train)\n",
    "\n",
    "                for epoch in tqdm(range(epochs), desc=f\"Station: {station}, epochs:\", leave=False):\n",
    "                    # Training the model\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    # Making predictions on the test set\n",
    "                    y_pred = module(X_train)\n",
    "\n",
    "                    y_pred = torch.maximum(y_pred, torch.tensor(threshold))\n",
    "                    # Evaluating the model\n",
    "                    loss = mean_absolute_percentage_error(y_train, y_pred, threshold)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Update the progress bar with the current loss\n",
    "                    tqdm.write(f'Station: {station}, dataset_size = {len(X_train)}, Epoch {epoch}, Loss: {loss:.4f}', end='\\r') # line 1\n",
    "                    #tqdm.write(f'{loss}', end='\\r') # line 1\n",
    "                    \n",
    "                    if (epoch+1) % scheduler_period == 0 :\n",
    "                        pass #scheduler.step()\n",
    "                \n",
    "                ## Code score on test_set and maybe something that takes best model like in ALTEGRAD project. ##\n",
    "                \n",
    "                test_pred = module(X_test)\n",
    "                print('Score on test set :', mean_absolute_percentage_error(y_test, test_pred, threshold) )\n",
    "            #self.save_small_data(self, path= f\"{k}\")\n",
    "            break\n",
    "\n",
    " \n",
    "    def test(self, test_data_init: pd.DataFrame):\n",
    "        \n",
    "        for model in self.models : \n",
    "            model.eval_mode()\n",
    "\n",
    "        test_data = test_data_init.copy()\n",
    "\n",
    "        test_data_features = test_data[self.features_col].values\n",
    "\n",
    "        # Careful using the same scaler that was fitted on the training set\n",
    "        X_test_normalized = self.scaler.transform(test_data_features)\n",
    "\n",
    "        y_pred = self.model.predict(X_test_normalized)\n",
    "        \n",
    "        y_pred = np.maximum(y_pred, 0)\n",
    "        test_data[self.label_col] = y_pred\n",
    "        self.submission = test_data[['index', 'y']]\n",
    "        return test_data\n",
    "    \n",
    "    \"\"\"def save_small_data(self, model: nn.Module, name = \"avg_model\"):\n",
    "        print('validation loss improved saving checkpoint...')\n",
    "        save_path =\n",
    "        \n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\"\"\"\n",
    "\n",
    "    def load(self, name=\"avg_model\"):\n",
    "       pass\n",
    "\n",
    "    def save_submission(self, name = \"submission\"):\n",
    "        self.submission.to_csv(\"../submissions/\" + name + \".csv\", index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données d'entraînement\n",
    "train_data = pd.read_csv('data/train_data_day_typed_cov_replaced_all_features.csv')\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Work only on 2019-2022 included\n",
    "train_data = date_filter(train_data, start_date='2019-12-31', end_date='2022-12-31')\n",
    "\n",
    "#one_st_filter = make_station_filter(train_data, stations=['1J7'])\n",
    "#train_data_one_station = train_data[one_st_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bbaaf9ebce40bc9678ebc8fc387c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split dataframes: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa35e9fa03e4ba8bd81d75a2eab13a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stations:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919235a04ad46c0ae278f13d0a2a34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Station: 003, epochs::   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station: 003, dataset_size = 782, Epoch 945, Loss: 379.2182\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot release un-acquired lock",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:624\u001b[0m, in \u001b[0;36mtqdm.external_write_mode\u001b[0;34m(cls, file, nolock)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# Clear all bars\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:651\u001b[0m, in \u001b[0;36mtqdm.get_lock\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the global lock. Construct it if it does not exist.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_lock \u001b[38;5;241m=\u001b[39m TqdmDefaultWriteLock()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m regressor \u001b[38;5;241m=\u001b[39m NeuralNetworkModelPytorch(train_data, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m, in \u001b[0;36mNeuralNetworkModelPytorch.train\u001b[0;34m(self, epochs, test_size, lr, Optim, print_period, threshold)\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Update the progress bar with the current loss\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStation: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstation\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, dataset_size = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, Loss: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mloss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# line 1\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#tqdm.write(f'{loss}', end='\\r') # line 1\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m scheduler_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:608\u001b[0m, in \u001b[0;36mtqdm.write\u001b[0;34m(cls, s, file, end, nolock)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Print a message via tqdm (without overlap with bars).\"\"\"\u001b[39;00m\n\u001b[1;32m    607\u001b[0m fp \u001b[38;5;241m=\u001b[39m file \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m--> 608\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_write_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnolock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnolock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Write the message\u001b[39;49;00m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:641\u001b[0m, in \u001b[0;36mtqdm.external_write_mode\u001b[0;34m(cls, file, nolock)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m--> 641\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:107\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.release\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelease\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocks[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# Release in inverse order of acquisition\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m         \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot release un-acquired lock"
     ]
    }
   ],
   "source": [
    "regressor = NeuralNetworkModelPytorch(train_data, size=10, hidden_dim=512, n_layers=2)\n",
    "regressor.train(epochs= int(1e3), Optim=optim.Adam, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_numeric</th>\n",
       "      <th>station</th>\n",
       "      <th>day_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01_1J7</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>52</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6</td>\n",
       "      <td>1J7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2023-02-01_1J7</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>2023-03-01_1J7</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>9</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2</td>\n",
       "      <td>1J7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>2023-04-01_1J7</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>13</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>5</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2023-05-01_1J7</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>18</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>1J7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76483</th>\n",
       "      <td>2023-05-30_1J7</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>22</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76919</th>\n",
       "      <td>2023-06-30_1J7</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>26</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77356</th>\n",
       "      <td>2023-01-31_1J7</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>5</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77777</th>\n",
       "      <td>2023-03-31_1J7</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>13</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78214</th>\n",
       "      <td>2023-05-31_1J7</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>22</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2</td>\n",
       "      <td>1J7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                index       date  week   day_name  day_numeric station   \n",
       "0      2023-01-01_1J7 2023-01-01    52     Sunday            6     1J7  \\\n",
       "432    2023-02-01_1J7 2023-02-01     5  Wednesday            2     1J7   \n",
       "870    2023-03-01_1J7 2023-03-01     9  Wednesday            2     1J7   \n",
       "1307   2023-04-01_1J7 2023-04-01    13   Saturday            5     1J7   \n",
       "1737   2023-05-01_1J7 2023-05-01    18     Monday            0     1J7   \n",
       "...               ...        ...   ...        ...          ...     ...   \n",
       "76483  2023-05-30_1J7 2023-05-30    22    Tuesday            1     1J7   \n",
       "76919  2023-06-30_1J7 2023-06-30    26     Friday            4     1J7   \n",
       "77356  2023-01-31_1J7 2023-01-31     5    Tuesday            1     1J7   \n",
       "77777  2023-03-31_1J7 2023-03-31    13     Friday            4     1J7   \n",
       "78214  2023-05-31_1J7 2023-05-31    22  Wednesday            2     1J7   \n",
       "\n",
       "       day_type  \n",
       "0             7  \n",
       "432           0  \n",
       "870           1  \n",
       "1307          0  \n",
       "1737          8  \n",
       "...         ...  \n",
       "76483         0  \n",
       "76919         0  \n",
       "77356         0  \n",
       "77777         0  \n",
       "78214         0  \n",
       "\n",
       "[175 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test_data_day_typed_cov_replaced_all_features.csv')\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "one_st_filter2 = make_station_filter(test_data, stations=['1J7'])\n",
    "test_data_one_station = test_data[one_st_filter2]\n",
    "test_data_one_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'eval_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_one_station\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m result\n",
      "Cell \u001b[0;32mIn[46], line 118\u001b[0m, in \u001b[0;36mRegressorModelPytorch.test\u001b[0;34m(self, test_data_init)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_data_init: pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels : \n\u001b[0;32m--> 118\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_mode\u001b[49m()\n\u001b[1;32m    120\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m test_data_init\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    122\u001b[0m     test_data_features \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_col]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'eval_mode'"
     ]
    }
   ],
   "source": [
    "result = regressor.test(test_data_init=test_data_one_station)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01_1J7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2023-02-01_1J7</td>\n",
       "      <td>159.875167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>2023-03-01_1J7</td>\n",
       "      <td>150.028338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>2023-04-01_1J7</td>\n",
       "      <td>84.714748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2023-05-01_1J7</td>\n",
       "      <td>130.474052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76483</th>\n",
       "      <td>2023-05-30_1J7</td>\n",
       "      <td>185.984068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76919</th>\n",
       "      <td>2023-06-30_1J7</td>\n",
       "      <td>110.608986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77356</th>\n",
       "      <td>2023-01-31_1J7</td>\n",
       "      <td>185.071749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77777</th>\n",
       "      <td>2023-03-31_1J7</td>\n",
       "      <td>109.911330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78214</th>\n",
       "      <td>2023-05-31_1J7</td>\n",
       "      <td>160.787486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                index           y\n",
       "0      2023-01-01_1J7    0.000000\n",
       "432    2023-02-01_1J7  159.875167\n",
       "870    2023-03-01_1J7  150.028338\n",
       "1307   2023-04-01_1J7   84.714748\n",
       "1737   2023-05-01_1J7  130.474052\n",
       "...               ...         ...\n",
       "76483  2023-05-30_1J7  185.984068\n",
       "76919  2023-06-30_1J7  110.608986\n",
       "77356  2023-01-31_1J7  185.071749\n",
       "77777  2023-03-31_1J7  109.911330\n",
       "78214  2023-05-31_1J7  160.787486\n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
